{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QVIM-AES Submission Template\n",
    "\n",
    "This is the submission template for the Query by Vocal Imitation challenge at the 2025 AES International Conference on Artificial Intelligence and Machine Learning for Audio.\n",
    "\n",
    "The content of this notebook is inspired by the template provided by the task organizers of the [Sound Scene Synthesis Taks of the DCASE Challenge 2024](https://dcase.community/challenge2024/task-sound-scene-synthesis).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>Confidentiality Statement</b><br> As the organizers of this contest, we assure all participants that their submitted models and code will be treated with strict confidentiality. Submissions will only be accessed by the designated review team for evaluation purposes and will not be shared, distributed, or used beyond the scope of this challenge. Participants retain full ownership of their work. We will not claim any rights over the submitted materials, nor will we use them for any purpose outside of the challenge evaluation process. We appreciate your participation in this challenge.\n",
    "</div>\n",
    "\n",
    "#### How to create your submission\n",
    "- Get familiar with the existing code blocks and the example provided below.\n",
    "- Set the root path of your environment and your dataset below (\"TODO: DEFINE YOUR PATHS HERE.\").\n",
    "- Set up your project (\"TODO: SETUP YOUR PROJECT HERE.\").\n",
    "- Implement the retrieval interface below (\"TODO: ADD YOUR IMPLEMENTATION HERE.\").\n",
    "    - Use the provided helper functions (helpers) to download your source code, model checkpoints, etc.\n",
    "- Instantiate your retrieval model (\"TODO: INSTANTIATE YOUR MODEL HERE.\").\n",
    "- Before **submitting your notebook**, run this notebook in a clean conda environment (with python >= 3.10) on Ubuntu 24.04 and make sure the evaluation results are in line with your previous results.\n",
    "- Submit your notebooks and the technical report as described on our [website](https://qvim-aes.github.io/).\n",
    "\n",
    "##### Some Rules\n",
    "- DO NOT modify the other code cells.\n",
    "- DO NOT add new cells.\n",
    "- Store your project WITHIN 'ROOT_PATH' and your data within 'DATA_PATH'.\n",
    "- DO NOT use 'ROOT_PATH/output' folder; this is where we will store things.\n",
    "- DO NOT change the working directory (e.g., `os.chdir('/path/to/a/dir/that/does/not/exist/on/my/machine')`).\n",
    "- DO NOT use system commands (`!cd ~` or `os.system('cd ~')`, etc.) other than the ones used to set up your environment (i.e., install required packages with pip, conda, ...).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> \n",
    "Participant who submit malicious code will be disqualified.\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.523366Z",
     "start_time": "2025-04-05T01:07:56.518678Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "# Install basic packages for template notebook.\n",
    "! pip install librosa numpy pandas tqdm GitPython gdown==5.1.0"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: numpy in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (2.1.3)\r\n",
      "Requirement already satisfied: pandas in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: tqdm in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: GitPython in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (3.1.44)\r\n",
      "Requirement already satisfied: gdown==5.1.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (5.1.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from gdown==5.1.0) (4.12.3)\r\n",
      "Requirement already satisfied: filelock in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from gdown==5.1.0) (3.18.0)\r\n",
      "Requirement already satisfied: requests[socks] in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from gdown==5.1.0) (2.32.3)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (3.0.1)\r\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (0.61.0)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (1.15.2)\r\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (1.6.1)\r\n",
      "Requirement already satisfied: joblib>=1.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (1.4.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (5.1.1)\r\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (0.13.1)\r\n",
      "Requirement already satisfied: pooch>=1.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (1.8.2)\r\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\r\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (4.12.2)\r\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (0.4)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from librosa) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from GitPython) (4.0.12)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.2)\r\n",
      "Requirement already satisfied: packaging in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from lazy_loader>=0.1->librosa) (24.2)\r\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.44.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.10.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from beautifulsoup4->gdown==5.1.0) (2.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from requests[socks]->gdown==5.1.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from requests[socks]->gdown==5.1.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from requests[socks]->gdown==5.1.0) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from requests[socks]->gdown==5.1.0) (2025.1.31)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from requests[socks]->gdown==5.1.0) (1.7.1)\r\n",
      "Requirement already satisfied: pycparser in /home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.722237Z",
     "start_time": "2025-04-05T01:07:57.533489Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "# some imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNASsBThlM2s"
   },
   "source": [
    "## Description of the Retrieval Interface \n",
    "`QVIMModel` is the interface specification for all query by vocal imitation systems. Each submitted system is expected to subclass this interface and implement the `compute_similarities` method, which computes the similarities between all pairwise combinations of queries (vocal imitations) and items (reference sounds).\n",
    "\n",
    "`compute_similarities` takes two dictionaries as input:\n",
    "- queries is a dictionary mapping ids of items to be retrieved to the corresponding file paths.\n",
    "- items is a dictionary mapping query ids to the corresponding file paths\n",
    "\n",
    "Participants are expected to load the sounds themselves, e.g., with `librosa.load`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "008R-IAWX0C5",
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.793484Z",
     "start_time": "2025-04-05T01:07:57.790494Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "class QVIMModel(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_similarities(\n",
    "            self, items: dict[str, str], queries: dict[str, str]\n",
    "    ) -> dict[str, dict[str, float]]:\n",
    "        \"\"\"Compute similarity scores between items to be retrieved and a set of queries.\n",
    "\n",
    "        Each <query, item> pairing should be assigned a single floating point score, where higher\n",
    "        scores indicate higher similarity.\n",
    "\n",
    "        Args:\n",
    "            items (dict[str, str]): A dictionary mapping ids of items to be retrieved to the corresponding file path\n",
    "            queries (dict[str, str]): A dictionary mapping query ids to the corresponding file path\n",
    "\n",
    "        Returns:\n",
    "            scores (dict[str, dict[str, float]]): A dictionary mapping query ids to a dictionary of item\n",
    "                ids and their corresponding similarity scores. E.g:\n",
    "                {\n",
    "                    \"query_1\": {\n",
    "                        \"item_1\": 0.8,\n",
    "                        \"item_2\": 0.6,\n",
    "                        ...\n",
    "                    },\n",
    "                    \"query_2\": {\n",
    "                        \"item_1\": 0.4,\n",
    "                        \"item_2\": 0.9,\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVKQD14BnoSd"
   },
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`helpers.py` contains some helpful functions for downloading code and model checkpoints from Google Drive, Git and public links.\n",
    "\n",
    "The functions were taken (with slight modifications) from the submission template provided by the task organizers of [Task 7 of the DCASE Challenge 2024: Sound Scene Synthesis](https://dcase.community/challenge2024/task-sound-scene-synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.902769Z",
     "start_time": "2025-04-05T01:07:57.799703Z"
    }
   },
   "source": [
    "import helpers\n",
    "from helpers import google_drive_download, wget_download, git_clone_checkout, unpack_file"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup your paths\n",
    "\n",
    "- Define `ROOT_PATH`; this is where your project lives; for testing, we'll replace it with our custom ROOT_PATH. We recommend using the current working directory ('.').\n",
    "- Define `DATA_PATH`; this is where your public development data lives; for testing, we'll replace it with our custom DATA_PATH. We recommend using 'data/qvim-dev'.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.910317Z",
     "start_time": "2025-04-05T01:07:57.908291Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "TODO: DEFINE YOUR PATHS HERE.\n",
    "\"\"\"\n",
    "\n",
    "# replace this with your custom ROOT_PATH; this is where your code/ checkpoints will be downloaded to\n",
    "ROOT_PATH = \".\"\n",
    "\n",
    "# path to the evaluation data; can be in ROOT_PATH\n",
    "DATA_PATH = os.path.join(\"data\", \"qvim-dev\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.962831Z",
     "start_time": "2025-04-05T01:07:57.959787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "helpers.ROOT_PATH = ROOT_PATH\n",
    "os.makedirs(ROOT_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "sys.path.append(os.path.join(ROOT_PATH))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Setup your environment, download checkpoints, etc.\n",
    "\n",
    "Setup your project and install the required packages here.\n",
    "The easiest way is to:\n",
    "1) convert your implementation into a package,\n",
    "2) clone the repository and checkout the specific branch and commit,\n",
    "3) install your package with pip install -e name_of_your_fancy_package\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Make sure your link to the repository and other URLs are publicly available.\n",
    "- Use **shared public URLs** (e.g. a shared Google Drive, Dropbox, Zenodo link) to download checkpoints into `ROOT_PATH`.\n",
    "- Use the provided helper functions (`google_drive_download`, `wget_download`, `git_clone_checkout`, and `unpack_file`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2FA2V1lYC3fy"
   },
   "source": [
    "\"\"\"\n",
    "TODO: SETUP YOUR PROJECT HERE.\n",
    "\"\"\"\n",
    "\n",
    "# clone repository\n",
    "git_clone_checkout(output_dir='qvim-baseline', url='https://github.com/qvim-aes/qvim-baseline.git', branch='main', commit_sha='cd2c1a314216996dcac46b0076008ecb81953bab')\n",
    "\n",
    "# install as a package\n",
    "!pip install -e qvim-baseline\n",
    "\n",
    "# download checkpoint\n",
    "wget_download(\n",
    "    filename=\"baseline.ckpt\",\n",
    "    shared_url=\"https://cloud.cp.jku.at/index.php/s/ScDdSP9bHEDmYtr/download/baseline.ckpt\",\n",
    "    relative_dir=\"resources\"\n",
    ")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed successfully: baseline.ckpt.\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Implement the QVIMModel Interface"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:00.959320Z",
     "start_time": "2025-04-05T01:10:58.166730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "TODO: ADD YOUR IMPLEMENTATION HERE.\n",
    "\"\"\"\n",
    "\n",
    "### Import packages or code here.\n",
    "import numpy as np\n",
    "from qvim_mn_baseline.ex_qvim import QVIMModule\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "### Implement the interface\n",
    "class MobileNetV3(QVIMModel):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        self.qvim_model = QVIMModule.load_from_checkpoint(\n",
    "            os.path.join(\"resources\", \"baseline.ckpt\")\n",
    "        )\n",
    "        self.qvim_model = self.qvim_model.eval()\n",
    "\n",
    "        self.config = self.qvim_model.config\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.qvim_model = self.qvim_model.cuda()\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        audio = librosa.load(\n",
    "            file_path,\n",
    "            sr=self.config.sample_rate,\n",
    "            mono=True,\n",
    "            duration=self.config.duration\n",
    "        )[0]\n",
    "\n",
    "        fixed_length = int(self.config.sample_rate  * self.config.duration)\n",
    "        array = np.zeros(fixed_length, dtype=\"float32\")\n",
    "        if len(audio) < fixed_length:\n",
    "            array[:len(audio)] = audio\n",
    "        if len(audio) >= fixed_length:\n",
    "            array[:fixed_length]  = audio[:fixed_length]\n",
    "\n",
    "        return torch.from_numpy(array).unsqueeze(0).to(self.qvim_model.device)\n",
    "\n",
    "    def embed_item(self, file_path):\n",
    "        with torch.no_grad():\n",
    "            return self.qvim_model.forward_reference(self.load_audio(file_path)).detach().cpu().numpy()[0]\n",
    "\n",
    "    def embed_query(self, file_path):\n",
    "        with torch.no_grad():\n",
    "            return self.qvim_model.forward_imitation(self.load_audio(file_path)).detach().cpu().numpy()[0]\n",
    "\n",
    "    def compute_similarities(\n",
    "            self, items: dict[str, str], queries: dict[str, str]\n",
    "    ):\n",
    "        scores = {key: {} for key in queries}\n",
    "\n",
    "        self.items_embs = {key: self.embed_item(val) for key, val in tqdm(items.items(), desc=\"Processing recordings\")}\n",
    "        self.query_embs = {key: self.embed_query(val) for key, val in tqdm(queries.items(), desc=\"Processing queries\")}\n",
    "\n",
    "        for query_name, query_emb in tqdm(self.query_embs.items(), desc=\"Calculating similarities\"):\n",
    "            for recording_name, recording_emb in self.items_embs.items():\n",
    "                sim = np.dot(recording_emb, query_emb)\n",
    "                scores[query_name][recording_name] = sim.item()\n",
    "\n",
    "        return scores"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Create an Instance of your QVIMModel"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:01.475404Z",
     "start_time": "2025-04-05T01:11:00.964734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "TODO: INSTANTIATE YOUR MODEL HERE.\n",
    "\"\"\"\n",
    "# instantiate and store your model in variable `QBVIM_MODEL`\n",
    "QBVIM_MODEL = MobileNetV3()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/qvim-submission/lib/python3.10/site-packages/torchvision/ops/misc.py:120: UserWarning: Don't use ConvNormActivation directly, please use Conv2dNormActivation and Conv3dNormActivation instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dICiJ1tOm4Vh"
   },
   "source": [
    "## Create Predictions\n",
    "\n",
    "To run this, download the development dataset and store them in `DATA_PATH`."
   ]
  },
  {
   "metadata": {
    "id": "flFJzBKtX2cw",
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:01.495605Z",
     "start_time": "2025-04-05T01:11:01.484786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "items_path = os.path.join(DATA_PATH, \"Items\")\n",
    "item_files = pd.DataFrame({'path': list(glob(os.path.join(items_path, \"**\", \"*.wav\"), recursive=True))})\n",
    "item_files[\"Class\"] = item_files['path'].transform(lambda x: x.split(os.path.sep)[-2])\n",
    "item_files[\"Items\"] = item_files['path'].transform(lambda x: x.split(os.path.sep)[-1])\n",
    "\n",
    "queries_path = os.path.join(DATA_PATH, \"Queries\")\n",
    "query_files = pd.DataFrame({'path': list(glob(os.path.join(queries_path, \"**\", \"*.wav\"), recursive=True))})\n",
    "query_files[\"Class\"] = query_files['path'].transform(lambda x: x.split(os.path.sep)[-2])\n",
    "query_files[\"Query\"] = query_files['path'].transform(lambda x: x.split(os.path.sep)[-1])\n",
    "\n",
    "print(\"Total item files:\", len(item_files))\n",
    "print(\"Total query files:\", len(query_files))\n",
    "\n",
    "if len(query_files) == 0 or len(item_files) == 0:\n",
    "    raise ValueError(\"No query files found! Download the development dataset and store it in 'DATA_PATH'.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total item files: 125\n",
      "Total query files: 985\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:13.073160Z",
     "start_time": "2025-04-05T01:11:01.525741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "scores = QBVIM_MODEL.compute_similarities(\n",
    "    items = {row[\"Items\"]: row[\"path\"] for i, row in item_files.iterrows()},\n",
    "    queries = {row[\"Query\"]: row[\"path\"] for i, row in query_files.iterrows()}\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing recordings: 100%|██████████| 124/124 [00:02<00:00, 47.04it/s] \n",
      "Processing queries: 100%|██████████| 985/985 [00:08<00:00, 112.64it/s]\n",
      "Calculating similarities: 100%|██████████| 985/985 [00:00<00:00, 7989.07it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:13.229554Z",
     "start_time": "2025-04-05T01:11:13.087740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "os.makedirs(os.path.join(ROOT_PATH, \"output\"), exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"output\", \"similarities.json\"), \"w\") as f:\n",
    "    json.dump(scores, f)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation on the Public Development Set\n",
    "\n",
    "Computes the Reciprocal Rank (RR) for each query in the public development set. The RR is the inverted rank $r_i$ of the correct item for query $i$. Submissions will be ranked via the Mean Reciprocal Randk (MRR) of queries $Q$ on a hidden test set:\n",
    "\n",
    "$$MRR = \\frac{1}{\\lvert Q \\rvert} \\sum_{i=1}^{\\lvert Q\\rvert} \\frac{1}{r_i}$$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:13.426Z",
     "start_time": "2025-04-05T01:11:13.236935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"output\", \"similarities.json\"), \"r\") as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "rankings = pd.DataFrame(dict(\n",
    "    **{ \"id\": [i for i in list(scores.keys())]},\n",
    "    **{ k: [v[k] for v in  scores.values() ] for k in scores[list(scores.keys())[0]].keys()}\n",
    ")).set_index(\"id\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, \"DEV Dataset.csv\"), skiprows=1\n",
    ")[['Label', 'Class', 'Items', 'Query 1', 'Query 2', 'Query 3']]\n",
    "\n",
    "df = df.melt(\n",
    "    id_vars=[col for col in df.columns if \"Query\" not in col],\n",
    "    value_vars=[\"Query 1\", \"Query 2\", \"Query 3\"],\n",
    "    var_name=\"Query Type\",\n",
    "    value_name=\"Query\"\n",
    ").dropna()\n",
    "\n",
    "# remove missing files\n",
    "rankings = rankings.loc[df[\"Query\"].unique(), df[\"Items\"].unique()]\n",
    "\n",
    "# load file with ground truth, i.e., query->item mapping; column 0 is item, colum 1 query\n",
    "ground_truth = {row['Query']: [row['Items']] for i, row in df.iterrows()}\n",
    "\n",
    "# find the rank of the correct item (real recording) for each query (imitation)\n",
    "position_of_correct = {}\n",
    "missing_query_files = []\n",
    "for query, correct_item_list in ground_truth.items():\n",
    "    # Skip if query is not in the DataFrame\n",
    "    if query not in rankings.index:\n",
    "        missing_query_files.append(query)\n",
    "        continue\n",
    "    # Get row and sort items by similarity in descending order\n",
    "    sorted_items = rankings.loc[query].sort_values(ascending=False)\n",
    "    # Find rank of correct items\n",
    "    position_of_correct[query] = {\n",
    "        item: sorted_items.index.get_loc(item) for item in correct_item_list if item in sorted_items.index\n",
    "    }\n",
    "    assert len(position_of_correct[query]) == len(correct_item_list), f\"Missing item! Got: {list(position_of_correct[query].keys())}. Expected: {correct_item_list}\"\n",
    "\n",
    "# compute MRR\n",
    "normalized_rrs = []\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    rr, irr = [], [] # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        rr.append(1 / (rank + 1))\n",
    "        irr.append(1 / (i + 1))\n",
    "    normalized_rrs.append(sum(rr) / sum(irr)) # normalize MRR with ideal one\n",
    "mrr = np.mean(normalized_rrs)\n",
    "\n",
    "print(\"Missing query files: \", len(missing_query_files))\n",
    "print(\"Missing item files: \", missing_query_files)\n",
    "print(\"MRR random:\", round((1/ np.arange(1,len(df[\"Items\"].unique()))).mean(), 4))\n",
    "print(\"MRR       :\", round(mrr, 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing query files:  0\n",
      "Missing item files:  []\n",
      "MRR random: 0.0447\n",
      "MRR       : 0.2726\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:17.136937Z",
     "start_time": "2025-04-05T01:11:13.450937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "ground_truth = {\n",
    "    row[\"Query\"]: [row_[\"Items\"] for j, row_ in df.drop_duplicates(\"Items\").iterrows() if row_[\"Class\"] == row[\"Class\"]] for i, row in df.drop_duplicates(\"Query\").iterrows()\n",
    "}\n",
    "\n",
    "position_of_correct = {}\n",
    "missing_query_files = []\n",
    "for query, correct_item_list in ground_truth.items():\n",
    "    # Skip if query is not in the DataFrame\n",
    "    if query not in rankings.index:\n",
    "        missing_query_files.append(query)\n",
    "        continue\n",
    "    # Get row and sort items by similarity in descending order\n",
    "    sorted_items = rankings.loc[query].sort_values(ascending=False)\n",
    "    # Find rank of correct items\n",
    "    position_of_correct[query] = {item: sorted_items.index.get_loc(item) for item in correct_item_list if item in sorted_items.index}\n",
    "    assert len(position_of_correct[query]) == len(correct_item_list), f\"Missing item!\"\n",
    "\n",
    "# compute MRR\n",
    "normalized_rrs = []\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    rr, irr = [], [] # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        rr.append(1 / (rank + 1))\n",
    "        irr.append(1 / (i + 1))\n",
    "    normalized_rrs.append(sum(rr) / sum(irr)) # normalize MRR with ideal one\n",
    "mrr = np.mean(normalized_rrs)\n",
    "\n",
    "# compute NDCG\n",
    "normalized_dcg = []\n",
    "ndcgs = {}\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    dcg, idcg = [], [] # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        dcg.append(1 / np.log2(rank + 2))\n",
    "        idcg.append(1 / np.log2(i + 2))\n",
    "    normalized_dcg.append(sum(dcg) / sum(idcg)) # normalize MRR with ideal one\n",
    "    ndcgs[query] = sum(dcg) / sum(idcg)\n",
    "ndcg = np.mean(normalized_dcg)\n",
    "\n",
    "print(\"Class-wise MRR :\", round(mrr, 4))\n",
    "print(\"Class-wise NDCG:\", round(ndcg, 4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise MRR : 0.4882\n",
      "Class-wise NDCG: 0.6463\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
