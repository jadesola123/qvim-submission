{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QVIM-AES Submission Template\n",
    "\n",
    "This is the submission template for the Query by Vocal Imitation challenge at the 2025 AES International Conference on Artificial Intelligence and Machine Learning for Audio.\n",
    "\n",
    "The content of this notebook is inspired by the template provided by the task organizers of the [Sound Scene Synthesis Taks of the DCASE Challenge 2024](https://dcase.community/challenge2024/task-sound-scene-synthesis).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>Confidentiality Statement</b><br> As the organizers of this contest, we assure all participants that their submitted models and code will be treated with strict confidentiality. Submissions will only be accessed by the designated review team for evaluation purposes and will not be shared, distributed, or used beyond the scope of this challenge. Participants retain full ownership of their work. We will not claim any rights over the submitted materials, nor will we use them for any purpose outside of the challenge evaluation process. We appreciate your participation in this challenge.\n",
    "</div>\n",
    "\n",
    "#### How to create your submission\n",
    "- Get familiar with the existing code blocks and the example provided below.\n",
    "- Set the root path of your environment and your dataset below (\"TODO: DEFINE YOUR PATHS HERE.\").\n",
    "- Set up your project (\"TODO: SETUP YOUR PROJECT HERE.\").\n",
    "- Implement the retrieval interface below (\"TODO: ADD YOUR IMPLEMENTATION HERE.\").\n",
    "    - Use the provided helper functions (helpers) to download your source code, model checkpoints, etc.\n",
    "- Instantiate your retrieval model (\"TODO: INSTANTIATE YOUR MODEL HERE.\").\n",
    "- Before **submitting your notebook**, run this notebook in a clean conda environment (with python >= 3.10) on Ubuntu 24.04 and make sure the evaluation results are in line with your previous results.\n",
    "- Submit your notebooks and the technical report as described on our [website](https://qvim-aes.github.io/).\n",
    "\n",
    "##### Some Rules\n",
    "- DO NOT modify the other code cells.\n",
    "- DO NOT add new cells.\n",
    "- Store your project WITHIN 'ROOT_PATH' and your data within 'DATA_PATH'.\n",
    "- DO NOT use 'ROOT_PATH/output' folder; this is where we will store things.\n",
    "- DO NOT change the working directory (e.g., `os.chdir('/path/to/a/dir/that/does/not/exist/on/my/machine')`).\n",
    "- DO NOT use system commands (`!cd ~` or `os.system('cd ~')`, etc.) other than the ones used to set up your environment (i.e., install required packages with pip, conda, ...).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> \n",
    "Participant who submit malicious code will be disqualified.\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "# Install basic packages for template notebook.\n",
    "! pip install librosa numpy pandas tqdm GitPython gdown==5.1.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "# some imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNASsBThlM2s"
   },
   "source": [
    "## Description of the Retrieval Interface \n",
    "`QVIMModel` is the interface specification for all query by vocal imitation systems. Each submitted system is expected to subclass this interface and implement the `compute_similarities` method, which computes the similarities between all pairwise combinations of queries (vocal imitations) and items (reference sounds).\n",
    "\n",
    "`compute_similarities` takes two dictionaries as input:\n",
    "- queries is a dictionary mapping ids of items to be retrieved to the corresponding file paths.\n",
    "- items is a dictionary mapping query ids to the corresponding file paths\n",
    "\n",
    "Participants are expected to load the sounds themselves, e.g., with `librosa.load`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "008R-IAWX0C5"
   },
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "class QVIMModel(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_similarities(\n",
    "            self, items: dict[str, str], queries: dict[str, str]\n",
    "    ) -> dict[str, dict[str, float]]:\n",
    "        \"\"\"Compute similarity scores between items to be retrieved and a set of queries.\n",
    "\n",
    "        Each <query, item> pairing should be assigned a single floating point score, where higher\n",
    "        scores indicate higher similarity.\n",
    "\n",
    "        Args:\n",
    "            items (dict[str, str]): A dictionary mapping ids of items to be retrieved to the corresponding file path\n",
    "            queries (dict[str, str]): A dictionary mapping query ids to the corresponding file path\n",
    "\n",
    "        Returns:\n",
    "            scores (dict[str, dict[str, float]]): A dictionary mapping query ids to a dictionary of item\n",
    "                ids and their corresponding similarity scores. E.g:\n",
    "                {\n",
    "                    \"query_1\": {\n",
    "                        \"item_1\": 0.8,\n",
    "                        \"item_2\": 0.6,\n",
    "                        ...\n",
    "                    },\n",
    "                    \"query_2\": {\n",
    "                        \"item_1\": 0.4,\n",
    "                        \"item_2\": 0.9,\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVKQD14BnoSd"
   },
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`helpers.py` contains some helpful functions for downloading code and model checkpoints from Google Drive, Git and public links.\n",
    "\n",
    "The functions were taken (with slight modifications) from the submission template provided by the task organizers of [Task 7 of the DCASE Challenge 2024: Sound Scene Synthesis](https://dcase.community/challenge2024/task-sound-scene-synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.902769Z",
     "start_time": "2025-04-05T01:07:57.799703Z"
    }
   },
   "source": [
    "import helpers\n",
    "from helpers import google_drive_download, wget_download, git_clone_checkout, unpack_file"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup your paths\n",
    "\n",
    "- Define `ROOT_PATH`; this is where your project lives; for testing, we'll replace it with our custom ROOT_PATH. We recommend using the current working directory ('.').\n",
    "- Define `DATA_PATH`; this is where your public development data lives; for testing, we'll replace it with our custom DATA_PATH. We recommend using 'data/qvim-dev'.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.910317Z",
     "start_time": "2025-04-05T01:07:57.908291Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "TODO: DEFINE YOUR PATHS HERE.\n",
    "\"\"\"\n",
    "\n",
    "# replace this with your custom ROOT_PATH; this is where your code/ checkpoints will be downloaded to\n",
    "ROOT_PATH = \".\"\n",
    "\n",
    "# path to the evaluation data; can be in ROOT_PATH\n",
    "DATA_PATH = os.path.join(\"data\", \"qvim-dev\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:07:57.962831Z",
     "start_time": "2025-04-05T01:07:57.959787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "helpers.ROOT_PATH = ROOT_PATH\n",
    "os.makedirs(ROOT_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "sys.path.append(os.path.join(ROOT_PATH))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Setup your environment, download checkpoints, etc.\n",
    "\n",
    "Setup your project and install the required packages here.\n",
    "The easiest way is to:\n",
    "1) convert your implementation into a package,\n",
    "2) clone the repository and checkout the specific branch and commit,\n",
    "3) install your package with pip install -e name_of_your_fancy_package\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Make sure your link to the repository and other URLs are publicly available.\n",
    "- Use **shared public URLs** (e.g. a shared Google Drive, Dropbox, Zenodo link) to download checkpoints into `ROOT_PATH`.\n",
    "- Use the provided helper functions (`google_drive_download`, `wget_download`, `git_clone_checkout`, and `unpack_file`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2FA2V1lYC3fy"
   },
   "source": [
    "\"\"\"\n",
    "TODO: SETUP YOUR PROJECT HERE.\n",
    "\"\"\"\n",
    "\n",
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Implement the QVIMModel Interface"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "TODO: ADD YOUR IMPLEMENTATION HERE.\n",
    "\"\"\"\n",
    "\n",
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Create an Instance of your QVIMModel"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "TODO: INSTANTIATE YOUR MODEL HERE.\n",
    "\"\"\"\n",
    "\n",
    "QBVIM_MODEL = None # store your model into this variable ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dICiJ1tOm4Vh"
   },
   "source": [
    "## Create Predictions\n",
    "\n",
    "To run this, download the development dataset and store them in `DATA_PATH`."
   ]
  },
  {
   "metadata": {
    "id": "flFJzBKtX2cw"
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "items_path = os.path.join(DATA_PATH, \"Items\")\n",
    "item_files = pd.DataFrame({'path': list(glob(os.path.join(items_path, \"**\", \"*.wav\"), recursive=True))})\n",
    "item_files[\"Class\"] = item_files['path'].transform(lambda x: x.split(os.path.sep)[-2])\n",
    "item_files[\"Items\"] = item_files['path'].transform(lambda x: x.split(os.path.sep)[-1])\n",
    "\n",
    "queries_path = os.path.join(DATA_PATH, \"Queries\")\n",
    "query_files = pd.DataFrame({'path': list(glob(os.path.join(queries_path, \"**\", \"*.wav\"), recursive=True))})\n",
    "query_files[\"Class\"] = query_files['path'].transform(lambda x: x.split(os.path.sep)[-2])\n",
    "query_files[\"Query\"] = query_files['path'].transform(lambda x: x.split(os.path.sep)[-1])\n",
    "\n",
    "print(\"Total item files:\", len(item_files))\n",
    "print(\"Total query files:\", len(query_files))\n",
    "\n",
    "if len(query_files) == 0 or len(item_files) == 0:\n",
    "    raise ValueError(\"No query files found! Download the development dataset and store it in 'DATA_PATH'.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "scores = QBVIM_MODEL.compute_similarities(\n",
    "    items = {row[\"Items\"]: row[\"path\"] for i, row in item_files.iterrows()},\n",
    "    queries = {row[\"Query\"]: row[\"path\"] for i, row in query_files.iterrows()}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:11:13.229554Z",
     "start_time": "2025-04-05T01:11:13.087740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "os.makedirs(os.path.join(ROOT_PATH, \"output\"), exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"output\", \"similarities.json\"), \"w\") as f:\n",
    "    json.dump(scores, f)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation on the Public Development Set\n",
    "\n",
    "Computes the Reciprocal Rank (RR) for each query in the public development set. The RR is the inverted rank $r_i$ of the correct item for query $i$. Submissions will be ranked via the Mean Reciprocal Randk (MRR) of queries $Q$ on a hidden test set:\n",
    "\n",
    "$$MRR = \\frac{1}{\\lvert Q \\rvert} \\sum_{i=1}^{\\lvert Q\\rvert} \\frac{1}{r_i}$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"output\", \"similarities.json\"), \"r\") as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "rankings = pd.DataFrame(dict(\n",
    "    **{ \"id\": [i for i in list(scores.keys())]},\n",
    "    **{ k: [v[k] for v in  scores.values() ] for k in scores[list(scores.keys())[0]].keys()}\n",
    ")).set_index(\"id\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, \"DEV Dataset.csv\"), skiprows=1\n",
    ")[['Label', 'Class', 'Items', 'Query 1', 'Query 2', 'Query 3']]\n",
    "\n",
    "df = df.melt(\n",
    "    id_vars=[col for col in df.columns if \"Query\" not in col],\n",
    "    value_vars=[\"Query 1\", \"Query 2\", \"Query 3\"],\n",
    "    var_name=\"Query Type\",\n",
    "    value_name=\"Query\"\n",
    ").dropna()\n",
    "\n",
    "# remove missing files\n",
    "rankings = rankings.loc[df[\"Query\"].unique(), df[\"Items\"].unique()]\n",
    "\n",
    "# load file with ground truth, i.e., query->item mapping; column 0 is item, colum 1 query\n",
    "ground_truth = {row['Query']: [row['Items']] for i, row in df.iterrows()}\n",
    "\n",
    "# find the rank of the correct item (real recording) for each query (imitation)\n",
    "position_of_correct = {}\n",
    "missing_query_files = []\n",
    "for query, correct_item_list in ground_truth.items():\n",
    "    # Skip if query is not in the DataFrame\n",
    "    if query not in rankings.index:\n",
    "        missing_query_files.append(query)\n",
    "        continue\n",
    "    # Get row and sort items by similarity in descending order\n",
    "    sorted_items = rankings.loc[query].sort_values(ascending=False)\n",
    "    # Find rank of correct items\n",
    "    position_of_correct[query] = {\n",
    "        item: sorted_items.index.get_loc(item) for item in correct_item_list if item in sorted_items.index\n",
    "    }\n",
    "    assert len(position_of_correct[query]) == len(correct_item_list), f\"Missing item! Got: {list(position_of_correct[query].keys())}. Expected: {correct_item_list}\"\n",
    "\n",
    "# compute MRR\n",
    "normalized_rrs = []\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    rr, irr = [], [] # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        rr.append(1 / (rank + 1))\n",
    "        irr.append(1 / (i + 1))\n",
    "    normalized_rrs.append(sum(rr) / sum(irr)) # normalize MRR with ideal one\n",
    "mrr = np.mean(normalized_rrs)\n",
    "\n",
    "print(\"Missing query files: \", len(missing_query_files))\n",
    "print(\"Missing item files: \", missing_query_files)\n",
    "print(\"MRR random:\", round((1/ np.arange(1,len(df[\"Items\"].unique()))).mean(), 4))\n",
    "print(\"MRR       :\", round(mrr, 4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS BLOCK.\n",
    "\"\"\"\n",
    "\n",
    "ground_truth = {\n",
    "    row[\"Query\"]: [row_[\"Items\"] for j, row_ in df.drop_duplicates(\"Items\").iterrows() if row_[\"Class\"] == row[\"Class\"]] for i, row in df.drop_duplicates(\"Query\").iterrows()\n",
    "}\n",
    "\n",
    "position_of_correct = {}\n",
    "missing_query_files = []\n",
    "for query, correct_item_list in ground_truth.items():\n",
    "    # Skip if query is not in the DataFrame\n",
    "    if query not in rankings.index:\n",
    "        missing_query_files.append(query)\n",
    "        continue\n",
    "    # Get row and sort items by similarity in descending order\n",
    "    sorted_items = rankings.loc[query].sort_values(ascending=False)\n",
    "    # Find rank of correct items\n",
    "    position_of_correct[query] = {item: sorted_items.index.get_loc(item) for item in correct_item_list if item in sorted_items.index}\n",
    "    assert len(position_of_correct[query]) == len(correct_item_list), f\"Missing item!\"\n",
    "\n",
    "# compute MRR\n",
    "normalized_rrs = []\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    rr, irr = [], [] # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        rr.append(1 / (rank + 1))\n",
    "        irr.append(1 / (i + 1))\n",
    "    normalized_rrs.append(sum(rr) / sum(irr)) # normalize MRR with ideal one\n",
    "mrr = np.mean(normalized_rrs)\n",
    "\n",
    "# compute NDCG\n",
    "normalized_dcg = []\n",
    "ndcgs = {}\n",
    "for query, items_ranks in position_of_correct.items():\n",
    "    dcg, idcg = [], [] # summed RR and ideal RR\n",
    "    for i, (item, rank) in enumerate(items_ranks.items()):\n",
    "        dcg.append(1 / np.log2(rank + 2))\n",
    "        idcg.append(1 / np.log2(i + 2))\n",
    "    normalized_dcg.append(sum(dcg) / sum(idcg)) # normalize MRR with ideal one\n",
    "    ndcgs[query] = sum(dcg) / sum(idcg)\n",
    "ndcg = np.mean(normalized_dcg)\n",
    "\n",
    "print(\"Class-wise MRR :\", round(mrr, 4))\n",
    "print(\"Class-wise NDCG:\", round(ndcg, 4))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
